{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0VT0sfUp36/5ggPC7ix/U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omkar1872/Assignment/blob/main/ASSIGNMENT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2 sentence-transformers transformers gradio sklearn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mtw6XVVdCTnB",
        "outputId": "7537f6f0-78df-4f32-fa3e-101637f163a2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (5.9.1)\n",
            "Collecting sklearn\n",
            "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "import json\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "\n",
        "# Function to extract text from a PDF file\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "# Example PDF paths\n",
        "pdf_paths = [\"/content/textbook1.pdf\", \"/content/textbook2.pdf\", \"/content/textbook3.pdf\"]\n",
        "textbooks = {f\"Textbook-{i+1}\": extract_text_from_pdf(path) for i, path in enumerate(pdf_paths)}\n",
        "\n",
        "# Function to build a hierarchical index for the textbooks\n",
        "def build_hierarchical_index(textbooks):\n",
        "    index = {}\n",
        "    for book, content in textbooks.items():\n",
        "        chapters = content.split(\"Chapter \")[1:]  # Assuming chapters are labeled as \"Chapter X\"\n",
        "        index[book] = {}\n",
        "        for chapter in chapters:\n",
        "            sections = chapter.split(\"Section \")[1:]  # Assuming sections are labeled as \"Section X\"\n",
        "            chapter_title = chapter.split(\"\\n\")[0].strip()\n",
        "            index[book][chapter_title] = {}\n",
        "            for section in sections:\n",
        "                section_title = section.split(\"\\n\")[0].strip()\n",
        "                paragraphs = section.split(\"\\n\")[1:]  # Split sections into paragraphs\n",
        "                index[book][chapter_title][section_title] = paragraphs\n",
        "    return index\n",
        "\n",
        "# Build the hierarchical index for textbooks\n",
        "hierarchical_index = build_hierarchical_index(textbooks)\n",
        "\n",
        "# Load Sentence-BERT model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n"
      ],
      "metadata": {
        "id": "HICrP4l7CUME"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import numpy as np\n",
        "\n",
        "# Initialize GPT-2 model for answer generation with increased max length\n",
        "llm = pipeline(\"text-generation\", model=\"gpt2\", max_new_tokens=500)\n",
        "\n",
        "# Implement BM25 and semantic retrieval methods\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import util\n",
        "\n",
        "def bm25_retrieval(query, paragraphs):\n",
        "    \"\"\"Implements BM25 retrieval using TF-IDF Vectorization.\"\"\"\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(paragraphs)\n",
        "    query_vec = vectorizer.transform([query])\n",
        "    scores = (tfidf_matrix * query_vec.T).toarray()\n",
        "    return scores.flatten()\n",
        "\n",
        "def semantic_retrieval(query, paragraphs, model):\n",
        "    \"\"\"Implements semantic retrieval using Sentence-BERT.\"\"\"\n",
        "    embeddings = model.encode(paragraphs, convert_to_tensor=True)\n",
        "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "    scores = util.pytorch_cos_sim(query_embedding, embeddings).cpu().numpy()\n",
        "    return scores.flatten()\n",
        "\n",
        "def hybrid_retrieval(query, paragraphs, model):\n",
        "    \"\"\"Combines BM25 and semantic retrieval.\"\"\"\n",
        "    bm25_scores = bm25_retrieval(query, paragraphs)\n",
        "    semantic_scores = semantic_retrieval(query, paragraphs, model)\n",
        "    combined_scores = bm25_scores + semantic_scores\n",
        "    return combined_scores\n",
        "\n",
        "# Function to retrieve and generate answers with content length limiting\n",
        "def retrieve_and_generate(query, index, model, llm, max_input_length=1000):\n",
        "    retrieved_content = []\n",
        "    context = []\n",
        "\n",
        "    for book, chapters in index.items():\n",
        "        for chapter, sections in chapters.items():\n",
        "            for section, paragraphs in sections.items():\n",
        "                combined_scores = hybrid_retrieval(query, paragraphs, model)\n",
        "                relevant_paragraphs = [p for i, p in enumerate(paragraphs) if combined_scores[i] > 0.1]\n",
        "\n",
        "                if relevant_paragraphs:\n",
        "                    retrieved_content.extend(relevant_paragraphs)\n",
        "                    context.append(f\"Book: {book}, Chapter: {chapter}, Section: {section}\")\n",
        "\n",
        "    if not retrieved_content:\n",
        "        return \"Sorry, I couldn't find relevant content for your question.\", []\n",
        "\n",
        "    # Limit the content length for the GPT model\n",
        "    context_text = \"\\n\".join(retrieved_content)\n",
        "    context_text = context_text[:max_input_length]  # Ensures the input text doesn't exceed max length\n",
        "\n",
        "    # Generate response with the limited content\n",
        "    response = llm(context_text)[0]['generated_text']\n",
        "\n",
        "    return response, context\n",
        "\n",
        "# Example usage\n",
        "query = \"What is Artificial Intelligence?\"\n",
        "response, context = retrieve_and_generate(query, hierarchical_index, model, llm)\n",
        "\n",
        "print(\"Answer:\", response)\n",
        "print(\"Context:\", context)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaQPEDrfCXf-",
        "outputId": "58203552-b12e-4d33-a10b-7f8504b5565e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: representations thatform thebasis ofsearch techniques. The detailed\n",
            "examples there ofstate- space andproblem- reduction representations will\n",
            "clarify what ismeant bywords likesearch andproblem solving inAI.\n",
            "Readers towhom thesubject ofsearch isnew areencouraged toturn\n",
            "tothose articles formore concrete presentations ofthefundamental\n",
            "ideas. \n",
            "andconceptually important class ofrepresentations.\n",
            "representations. Blind search algorithms, which treat thesearch space\n",
            "syntactically, arecontrasted with heuristic methods, which use infor-\n",
            "thesearch. Various search algorithms arepresented infull.\n",
            "22 Search II\n",
            "onsearch. Italso describes twoprograms, STRIPS andABSTRIPS, that\n",
            "introduce theclosely related topic ofplanning inproblem solving. This\n",
            "general topic, however, istreated more fully in\n",
            "ofasearch problem, canaproblem- solving system beprogrammed to\n",
            "find abetter representation automatically? The question differs from\n",
            "that ofthe first approach tolimiting search inthat here itisthe\n",
            "program, nottheprocedure ofsearch. It is also aprocedure offungal andsyntactically important subject ofworried aboutinimf. In all aheuristic\n",
            "ofsearch,it is well known thatthe search method can not be used tofindabetter representation. Onthat\n",
            "searchers can search withoutoblivious to thespace andproblem. Therefore,\n",
            "suchmethod is much better suited to solvespace andproblem.\n",
            "the fact thatwords are considered inadjectives and\n",
            "suggestive verbs ofword choice is nota clear answer for the\n",
            "suppression question: Doeswords\n",
            "can be used inadjectives and sugges-\n",
            "spectively? It is the same principle that has been\n",
            "substantiated andappeared in other articles, on the one\n",
            "which also says thatwords can beused inadjectives: it refers to the fact that\n",
            "\n",
            "they use bothwords inattentionally. Therefore, it is\n",
            "\n",
            "observable thatword is always aadjective inadjective: for\n",
            "\n",
            "to which words are notadjectives,they justuseadjective.\n",
            "informally\n",
            "\n",
            "thatword can be used inadjectives: it should not be read asaadjective.\n",
            "\n",
            "inadjective: 'as aadjective' as adverb,it is not read as 'asadjective' as adverb : it is read as a word\n",
            "\n",
            "(for example, 'as aadjective' is adverb 'as' ).\n",
            "\n",
            "In mypersonal and work experience, the word is adverb \"as\" rather\n",
            "\n",
            "than noun \"as\". The word can be used inadjectives. But, \"\" as it is written here isnot aadverb! If the word\n",
            "\n",
            "would be understood as a adverb that can be written in an adverb way.\n",
            "\n",
            "The expression is thus a question,but the issue ofword to be considered inadverb\n",
            "\n",
            "is not the question.In the search program,word must be the adverb ofwords : Itshould not be the\n",
            "\n",
            "substitution of words; it is the substitution ofwords used with\n",
            "\n",
            "and withoutwords in whichwords do not. In the\n",
            "\n",
            "program, it is true thatwords are never aadverb inadjectives; so, ifwords\n",
            "\n",
            "are used inadjectives,then\n",
            "Context: ['Book: Textbook-2, Chapter: II, Section: n.B, describe theproblem', 'Book: Textbook-2, Chapter: II, Section: n.B also discusses game trees, which areahistorically', 'Book: Textbook-2, Chapter: II, Section: n.Cdeals with thealgorithms that usethese various problem', 'Book: Textbook-2, Chapter: II, Section: n.Dreviews some well-known early programs based', 'Book: Textbook-2, Chapter: XVI, in, Section: n.C.', 'Book: Textbook-2, Chapter: XVI, inVolume m., Section: n.D.', 'Book: Textbook-2, Chapter: XVI, inVolume m., Section: n.C5. Aswith search inother domains, thesource', 'Book: Textbook-2, Chapter: XVI, inVolume m., Section: n.C3.', 'Book: Textbook-2, Chapter: XVI, inVolume m., Section: n.B, inanapproach generally following Nilsson', 'Book: Textbook-2, Chapter: 2ofPrincipia Mathe-, Section: m.Cgointo', 'Book: Textbook-2, Chapter: m., Section: IV.F, useextended grammar parsers.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import gradio as gr\n",
        "\n",
        "# Gradio interface setup with adjustments for longer responses\n",
        "def gradio_qa_interface(query):\n",
        "    response, context = retrieve_and_generate(query, hierarchical_index, model, llm)\n",
        "    context_text = \"\\n\".join(context)\n",
        "    return response, context_text\n",
        "\n",
        "# Define the Gradio UI\n",
        "gui = gr.Interface(\n",
        "    fn=gradio_qa_interface,\n",
        "    inputs=gr.Textbox(label=\"Enter your query:\"),\n",
        "    outputs=[gr.Textbox(label=\"Generated Answer:\"), gr.Textbox(label=\"Context Information:\")],\n",
        "    title=\"Hierarchical QA System\",\n",
        "    description=\"Ask questions about the textbooks. The system retrieves relevant content and generates detailed answers.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio UI\n",
        "gui.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "Wt60MmMXCkNS",
        "outputId": "8a0d62cc-f1d3-4bb9-b880-117e9e6bb99a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://2d018aa2c57d9d5f5d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2d018aa2c57d9d5f5d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DqiAalclFt9D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}